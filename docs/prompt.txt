I am making an unconventional percussion piece for an ensemble of 9 players using embedded sensor technology to wirelessly transmit signals to play audio. I need your help to make this project.

Context: I am in a college organization at UT-Austin called the Texas Society of Unconventional Drummers, where we perform a show for an audience every semester comprised of several musical percussive pieces using unconventional instruments, such as glasses, boomwhackers, buckets, metal bottles, and more. The theme of our current show is 'Time Travel', and the concept of my piece is the 'Future'. The performers are meant to act as androids who use advanced technology (the sensors) integrated into their body to play music. The sensors used will be used to detect when a human touches it. Based on the sensor contact, the audio system will then play a predetermined sound depending on which signal was transmitted. I have notated the piece in MuseScore. The players will control when the notes are played, but the dynamic at which they will be played and the audio sample they map to are set by me beforehand. 

Ensemble and instruments: The instruments used will be the touch sensors, which are meant to emulate a soundboard. Each player will have 6 sensors on their body, each mapped to a different sound. There will be one sensor on each shoulder, 2 on their torso, and 2 on their thigh area. Unless there is a better option, the sensors for each person will be wired to an embedded device in their back pocket that detects their current contact status and transmits the data to a receiver. The electronic component of the music will be the only sounds heard in the piece. That means there will be no physical percussion or body sounds played in the song. 

Sensor and technology setup: I currently need help finding sensors to use. They should be roughly hand-sized. They should use capacitive touch detect when a person contacts or holds it unless there's a better option. Each player will have 6 sensors on their body, wired to an embedded device that detects contact and transmits a signal to a receiver. To my understanding, custom RF is the best way to minimize input delay, so we should use that to transmit signals. Let me know if it is possible to use STM32 microcontrollers to manage this interaction because I have experience working with them. All signals should be routed to one receiver, as I would like to use the sound system of the auditorium we perform in to broadcast audio. I need help receiving and parsing the signals into the audio samples that I want to play for each person. Ideally, the receiver somehow transmits data to a program that detects which sensor was touched. Then, using that information, the program maps the signal to an audio sample which is then played through the sound system, which will be wired to the computer. My piece is extremely sensitive to timing precision. Try to maximize performance, sound quality, and affordability when making decisions. Let me know what tradeoffs there are. 

Audio processing and mapping: I still need help determining how to map the received signals to audio output. Let me know high performance ways to process the received signals to audio samples, preferably in the form of a soundfont file (.sf2), since that is what I use to write music in MuseScore. If it is simpler, I also have access to the raw .wav audio samples that I want to use, and I can manipulate how they sound using Audacity. Choose whichever option works best. Each sensor will trigger a sound that is set by me. Yes. I have selected the audio samples that I want to use. 

Practical considerations: The piece will be performed indoors on the stage of an auditorium. The distance of the wireless communication will be relatively short range, as the receiver should hopefully be somewhere near the stage. Let me know if there are concerns with conflicting signals from devices in the audience, potential latency issues with connecting to the auditorium's sound system, conflicting signals from the devices of other performers, or range issues with the wireless communication. I'm comfortable buying or building custom devices as long as they don't come with any extraneous features that result in a higher cost. I need to keep this project at a reasonable budget for a college student. This piece will be performed in about 3 months, but I need the instruments within a week or two so that I can start teaching my performers. If this isn't possible, let me know and we can possibly extend the deadline. 

I primarily need help planning the technical part of this piece. We need to prioritize low latency, high sound quality, and a reasonable cost. Give me detailed instructions on how to make this piece happen using the information I have given you.





I am making an unconventional percussion piece for an ensemble of 9 players using embedded sensor technology to wirelessly transmit signals to play audio. I need your help to make this project.



Context: I am in a college organization at UT-Austin called the Texas Society of Unconventional Drummers, where we perform a show for an audience every semester comprised of several musical percussive pieces using unconventional instruments, such as glasses, boomwhackers, buckets, metal bottles, and more. The theme of our current show is 'Time Travel', and the concept of my piece is the 'Future'. The performers are meant to act as androids who use advanced technology (the sensors) integrated into their body to play music. The sensors used will be used to detect when a human touches it. Based on the sensor contact, the audio system will then play a predetermined sound depending on which signal was transmitted. I have notated the piece in MuseScore. The players will control when the notes are played, but the dynamic at which they will be played and the audio sample they map to are set by me beforehand. 



Ensemble and instruments: The instruments used will be the touch sensors, which are meant to emulate a soundboard. Each player will have 6 sensors on their body, each mapped to a different sound. There will be one sensor on each shoulder, 2 on their torso, and 2 on their thigh area. Unless there is a better option, the sensors for each person will be wired to an embedded device in their back pocket that detects their current contact status and transmits the data to a receiver. The electronic component of the music will be the only sounds heard in the piece. That means there will be no physical percussion or body sounds played in the song. 



Sensor and technology setup: I currently need help finding sensors to use. They should be roughly hand-sized. They should use capacitive touch detect when a person contacts or holds it unless there's a better option. Each player will have 6 sensors on their body, wired to an embedded device that detects contact and transmits a signal to a receiver. To my understanding, custom RF is the best way to minimize input delay, so we should use that to transmit signals. Let me know if it is possible to use STM32 microcontrollers to manage this interaction because I have experience working with them. All signals should be routed to one receiver, as I would like to use the sound system of the auditorium we perform in to broadcast audio. I need help receiving and parsing the signals into the audio samples that I want to play for each person. Ideally, the receiver somehow transmits data to a program that detects which sensor was touched. Then, using that information, the program maps the signal to an audio sample which is then played through the sound system, which will be wired to the computer. My piece is extremely sensitive to timing precision. Try to maximize performance, sound quality, and affordability when making decisions. Let me know what tradeoffs there are. 



Audio processing and mapping: I still need help determining how to map the received signals to audio output. Let me know high performance ways to process the received signals to audio samples, preferably in the form of a soundfont file (.sf2), since that is what I use to write music in MuseScore. If it is simpler, I also have access to the raw .wav audio samples that I want to use, and I can manipulate how they sound using Audacity. Choose whichever option works best. Each sensor will trigger a sound that is set by me. Yes. I have selected the audio samples that I want to use. 



Practical considerations: The piece will be performed indoors on the stage of an auditorium. The distance of the wireless communication will be relatively short range, as the receiver should hopefully be somewhere near the stage. Let me know if there are concerns with conflicting signals from devices in the audience, potential latency issues with connecting to the auditorium's sound system, conflicting signals from the devices of other performers, or range issues with the wireless communication. I'm comfortable buying or building custom devices as long as they don't come with any extraneous features that result in a higher cost. I need to keep this project at a reasonable budget for a college student. This piece will be performed in about 3 months, but I need the instruments within a week or two so that I can start teaching my performers. If this isn't possible, let me know and we can possibly extend the deadline. 



I primarily need help planning the technical part of this piece. We need to prioritize low latency, high sound quality, and a reasonable cost. Give me detailed instructions on how to make this piece happen using the information I have given you.